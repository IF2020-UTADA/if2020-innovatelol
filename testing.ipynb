{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IEdNLIGxLE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "import math\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tme6NAFa1GcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "33261e3f-c3e4-43db-c1d7-3bff60fd4293"
      },
      "source": [
        "# Mount GoogleDrive:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59Q_2g61bdQ",
        "colab_type": "text"
      },
      "source": [
        "#1. Load images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfQfXsCA1IkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a0521b1-d330-448d-bc5d-2f3765cdad25"
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])\n",
        "\n",
        "# change the path based on where you put the image folder\n",
        "rootpath = \"/content/gdrive/My Drive/Testing cokes/no_bgr\"\n",
        "dataset = torchvision.datasets.ImageFolder(root=rootpath, transform=transform)\n",
        "print(\"Total number of images\", len(dataset))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of images 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQWOKKGouJBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = []\n",
        "for i in range (len(dataset)):\n",
        "  indices.append(i)\n",
        "\n",
        "np.random.seed(1000) # Fixed numpy random seed for reproducible shuffling\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "test_sampler = SubsetRandomSampler(indices)\n",
        "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=1, sampler=test_sampler)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5VTrrc71vQi",
        "colab_type": "text"
      },
      "source": [
        "# 2. Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpxO27z-jL-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def squash(inputs, axis=-1):\n",
        "    \"\"\"\n",
        "    The primary capsule will use this function to squash all the vectors\n",
        "    Meaning the magnitudes will be normalized while the the directions are preserved\n",
        "    \"\"\"\n",
        "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
        "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
        "    return scale * inputs\n",
        "\n",
        "def caps_loss(y_true, y_pred, x, x_recon, lam_recon):\n",
        "\n",
        "    L = y_true * torch.clamp(0.9 - y_pred, min=0.) ** 2 + \\\n",
        "        0.5 * (1 - y_true) * torch.clamp(y_pred - 0.1, min=0.) ** 2\n",
        "    L_margin = L.sum(dim=1).mean()\n",
        "\n",
        "    L_recon = nn.MSELoss()(x_recon, x)\n",
        "\n",
        "    return L_margin + lam_recon * L_recon\n",
        "    \n",
        "class DenseCapsule(nn.Module):\n",
        "\n",
        "    def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, routings=3):\n",
        "        super(DenseCapsule, self).__init__()\n",
        "        self.in_num_caps = in_num_caps\n",
        "        self.in_dim_caps = in_dim_caps\n",
        "        self.out_num_caps = out_num_caps\n",
        "        self.out_dim_caps = out_dim_caps\n",
        "        self.routings = routings\n",
        "        self.weight = nn.Parameter(0.01 * torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.size=[batch, in_num_caps, in_dim_caps]\n",
        "        # expanded to    [batch, 1,            in_num_caps, in_dim_caps,  1]\n",
        "        # weight.size   =[       out_num_caps, in_num_caps, out_dim_caps, in_dim_caps]\n",
        "        # torch.matmul: [out_dim_caps, in_dim_caps] x [in_dim_caps, 1] -> [out_dim_caps, 1]\n",
        "        # => x_hat.size =[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
        "        \n",
        "        compute = torch.matmul(self.weight, x[:, None, :, :, None])\n",
        "        x_hat = torch.squeeze(compute, dim=-1)\n",
        "\n",
        "        # In forward pass, `x_hat_detached` = `x_hat`;\n",
        "        # In backward, no gradient can flow from `x_hat_detached` back to `x_hat`.\n",
        "        x_hat_detached = x_hat.detach()\n",
        "\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.size = [batch, out_num_caps, in_num_caps]\n",
        "        b = Variable(torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)).cuda()\n",
        "\n",
        "        assert self.routings > 0, 'The \\'routings\\' should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.size = [batch, out_num_caps, in_num_caps]\n",
        "            c = F.softmax(b, dim=1)\n",
        "\n",
        "            # At last iteration, use `x_hat` to compute `outputs` in order to backpropagate gradient\n",
        "            if i == self.routings - 1:\n",
        "                # c.size expanded to [batch, out_num_caps, in_num_caps, 1           ]\n",
        "                # x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\n",
        "                # => outputs.size=   [batch, out_num_caps, 1,           out_dim_caps]\n",
        "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n",
        "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat))  # alternative way\n",
        "            else:  # Otherwise, use `x_hat_detached` to update `b`. No gradients flow on this path.\n",
        "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat_detached, dim=-2, keepdim=True))\n",
        "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat_detached))  # alternative way\n",
        "\n",
        "                # outputs.size       =[batch, out_num_caps, 1,           out_dim_caps]\n",
        "                # x_hat_detached.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
        "                # => b.size          =[batch, out_num_caps, in_num_caps]\n",
        "                b = b + torch.sum(outputs * x_hat_detached, dim=-1)\n",
        "\n",
        "        return torch.squeeze(outputs, dim=-2)\n",
        "\n",
        "\n",
        "class PrimaryCapsule(nn.Module):\n",
        "    \"\"\"\n",
        "    convert the feature maps from CNN to vectors and squash the vectors\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0):\n",
        "        super(PrimaryCapsule, self).__init__()\n",
        "        self.dim_caps = dim_caps\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.conv2d(x)\n",
        "        outputs = outputs.view(x.size(0), -1, self.dim_caps)\n",
        "        return squash(outputs)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41JFvLTVjbzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsuleNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, classes, routings):\n",
        "        super(CapsuleNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.classes = classes\n",
        "        self.routings = routings\n",
        "\n",
        "        # Layer 1: Just a conventional Conv2D layer\n",
        "        self.conv1 = nn.Conv2d(input_size[0], 16, kernel_size=3, stride=1, padding=0)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2) \n",
        "\n",
        "        # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_caps, dim_caps]\n",
        "        self.primarycaps = PrimaryCapsule(32, 32, 8, kernel_size=5, stride=2, padding=0)\n",
        "\n",
        "        # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "        self.digitcaps = DenseCapsule(in_num_caps=676, in_dim_caps=8,\n",
        "                                      out_num_caps=classes, out_dim_caps=16, routings=routings)\n",
        "\n",
        "        # Decoder network.\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(16*classes, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, input_size[0] * input_size[1] * input_size[2]),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        x = self.pool(self.conv1(x))\n",
        "        x = self.pool(self.conv2(x))\n",
        "        x = self.primarycaps(x)\n",
        "        x = self.digitcaps(x)\n",
        "        length = x.norm(dim=-1)\n",
        "        if y is None:  # during testing, no label given. create one-hot coding using `length`\n",
        "            index = length.max(dim=1)[1]\n",
        "            y = Variable(torch.zeros(length.size()).scatter_(1, index.view(-1, 1).cpu().data, 1.).cuda())\n",
        "        reconstruction = self.decoder((x * y[:, :, None]).view(x.size(0), -1))\n",
        "        return length, reconstruction.view(-1, *self.input_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W443-gjXqtzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in test_loader:\n",
        "        y = torch.zeros(y.size(0), 10).scatter_(1, y.view(-1, 1), 1.)\n",
        "        x, y = Variable(x.cuda(), volatile=True), Variable(y.cuda())\n",
        "        y_pred, x_recon = model(x)\n",
        "        test_loss += caps_loss(y, y_pred, x, x_recon, 0.0005 * 784).item()  # sum up batch loss\n",
        "        y_pred = y_pred.data.max(1)[1]\n",
        "        y_true = y.data.max(1)[1]\n",
        "        correct += y_pred.eq(y_true).sum().item()\n",
        "        total += x.shape[0]\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    return test_loss, correct/total\n",
        "\n",
        "\n",
        "def train(model, train_loader, test_loader, epoch, learning_rate):\n",
        "\n",
        "    print('Begin Training' + '-'*70)\n",
        "\n",
        "        # starts timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    lr_decay = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "    train_accs, val_accs, train_losses, val_losses, iters = [], [], [], [], []    \n",
        "    n=0\n",
        "\n",
        "    for epoch in range(epoch):\n",
        "        model.train()  # set to training mode\n",
        "        lr_decay.step()  # decrease the learning rate by multiplying a factor `gamma`\n",
        "        total_train_loss = 0.0\n",
        "        j = 0\n",
        "        for i, (x, y) in enumerate(train_loader):  # batch training\n",
        "            y = torch.zeros(y.size(0), 10).scatter_(1, y.view(-1, 1), 1.)  # change to one-hot coding\n",
        "            x, y = Variable(x.cuda()), Variable(y.cuda())  # convert input data to GPU Variable\n",
        "\n",
        "            optimizer.zero_grad()  # set gradients of optimizer to zero\n",
        "            y_pred, x_recon = model(x, y)  # forward\n",
        "            loss = caps_loss(y, y_pred, x, x_recon, 0.0005 * 784)  # compute loss\n",
        "            loss.backward()  # backward, compute all gradients of loss w.r.t all Variables\n",
        "            total_train_loss += loss.item()  # record the batch loss\n",
        "            optimizer.step()  # update the trainable parameters with computed gradients\n",
        "            j+=1\n",
        "\n",
        "        # compute validation loss and acc\n",
        "        val_loss, val_acc = test(model, val_loader)\n",
        "        train_loss, train_acc = test(model, train_loader)\n",
        "\n",
        "        # track accuracy\n",
        "        train_accs.append((train_acc))\n",
        "        val_accs.append((val_acc))\n",
        "        train_losses.append((total_train_loss) / (j+1))\n",
        "        #train_loss.append(loss)\n",
        "        val_losses.append(val_loss)\n",
        "        iters.append(n)\n",
        "        n += 1\n",
        "        print(\"epoch:\", n, \"train_acc:\", train_accs[-1], \"val_acc:\", val_accs[-1], \"train_loss:\", train_losses[-1], \"val_loss:\", val_losses[-1])\n",
        "    \n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "        plt.title(\"Train vs. Validation Accuracy\")\n",
        "        plt.plot(iters, train_accs, label=\"Train\")\n",
        "        plt.plot(iters, val_accs, label=\"Validation\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Training Accuracy\")\n",
        "        plt.legend(loc='best')\n",
        "        plt.show()\n",
        "\n",
        "        plt.title(\"Train vs. Validation Loss\")\n",
        "        plt.plot(iters, train_losses, label=\"Train\")\n",
        "        plt.plot(iters, val_losses, label=\"Validation\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Training Loss\")\n",
        "        plt.legend(loc='best')\n",
        "        plt.show()\n",
        "\n",
        "    print('Finished Training')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXLxMsUHucYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90e4ecf1-92f5-4ba1-d21c-1ef5c6cf461c"
      },
      "source": [
        "# make sure the this pickle file is in your working directory \n",
        "\n",
        "import pickle\n",
        "pklfile = open('primary_model.pkl', 'rb')\n",
        "mymodel = pickle.load(pklfile)\n",
        "pklfile.close"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function BufferedReader.close>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrJGul8L2fwB",
        "colab_type": "text"
      },
      "source": [
        "# 3. Visualize the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGLcpGajwAC0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1f96cc44-6279-4b17-8335-bcc357a48d2f"
      },
      "source": [
        "# this will get the loss and accuracy for the dataset \n",
        "\n",
        "print(\"loss:\", test(mymodel, test_loader)[0])\n",
        "print(\"accuracy:\", test(mymodel, test_loader)[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.38018367840693545\n",
            "accuracy: 0.5384615384615384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LWgkTkmu8I2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "02d6a208-eac1-485a-c390-c2d380d7cb6d"
      },
      "source": [
        "# view the true and prediction labels\n",
        "\n",
        "mymodel.train()\n",
        "pred = []\n",
        "label = []\n",
        "for i, (x, y) in enumerate(test_loader):  # batch training\n",
        "    y = torch.zeros(y.size(0), 10).scatter_(1, y.view(-1, 1), 1.)  # change to one-hot coding\n",
        "    x, y = Variable(x.cuda()), Variable(y.cuda())  # convert input data to GPU Variable\n",
        "    y_pred, x_recon = mymodel(x, y)  # forward\n",
        "    t_values, t_indice = torch.max(y[0], 0)\n",
        "    values, indice = torch.max(y_pred[0], 0)\n",
        "    a=indice.cpu().detach().numpy()\n",
        "    b=t_indice.cpu().detach().numpy()\n",
        "    label.append(int(b))\n",
        "    pred.append(int(a))\n",
        "\n",
        "classes = [\"socket\", \"remote control\", \"cellphone\", \"scissors\", \"bulb\", \"coke\", \"sunglasses\", \"ball\", \"highlighter\", \"cup\"]\n",
        "\n",
        "for i in range(0, 13):\n",
        "  print(\"image\", i+1)\n",
        "  print(\"true label\", classes[label[i]])\n",
        "  print(\"prediction\", classes[pred[i]])\n",
        "  print(\"\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image 1\n",
            "true label coke\n",
            "prediction highlighter\n",
            "\n",
            "image 2\n",
            "true label coke\n",
            "prediction ball\n",
            "\n",
            "image 3\n",
            "true label coke\n",
            "prediction coke\n",
            "\n",
            "image 4\n",
            "true label coke\n",
            "prediction highlighter\n",
            "\n",
            "image 5\n",
            "true label coke\n",
            "prediction coke\n",
            "\n",
            "image 6\n",
            "true label coke\n",
            "prediction coke\n",
            "\n",
            "image 7\n",
            "true label coke\n",
            "prediction coke\n",
            "\n",
            "image 8\n",
            "true label coke\n",
            "prediction coke\n",
            "\n",
            "image 9\n",
            "true label coke\n",
            "prediction coke\n",
            "\n",
            "image 10\n",
            "true label coke\n",
            "prediction ball\n",
            "\n",
            "image 11\n",
            "true label coke\n",
            "prediction ball\n",
            "\n",
            "image 12\n",
            "true label coke\n",
            "prediction coke\n",
            "\n",
            "image 13\n",
            "true label coke\n",
            "prediction socket\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}